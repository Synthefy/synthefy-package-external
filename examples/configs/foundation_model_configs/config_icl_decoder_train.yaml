device: "cuda:0"
task: "forecast"
trainer: "foundation_model"
save_key: "val_loss" #"val_loss"

# history_length = time_series_length - forecast_length
dataset_config:
  dataloader_name: OTFSyntheticDataloader # V3ShardedDataloader
  dataset_name: synthetic_tabular # haver4M_synthefy250K
  v3_data_paths: ["/mnt/workspace4/gift_eval_test_shards/restaurant/blind/"]
  prior_config_path: "src/synthefy_pkg/prior/config/synthetic_configs/config_tiny_series.yaml"
  num_channels: 1
  time_series_length: 512 # 768
  forecast_length: 128
  metadata_length: 3460
  timestamp_start_idx: 3
  timestamp_end_idx: 2819
  dataset_description_start_idx: 2819
  dataset_description_end_idx: 3203
  continuous_start_idx: 3203
  continuous_end_idx: 3459
  dataset_idx_start_idx: 3459
  dataset_idx_end_idx: 3460
  text_embedding_dim: 384
  batch_size: 2
  accumulate_grad_batches: 32
  num_timestamp_features: 11
  num_correlates: 46
  num_datasets: 10000
  use_relation_shards: false
  using_synthetic_data: true
  num_workers: 16
  is_regression: true
  is_tabular: false
  # curriculum_config_path: "src/synthefy_pkg/prior/config/curriculum_configs/config_adaptive_curr.yaml"
  mixed_real_synthetic_sampling: false

# 170752
foundation_model_config:
  model_name: "multilayer_tabicl" #"tabicl" # "synthefy_foundation_forecasting_model_v3e"
  # model_name: "tabicl" #"tabicl" # "synthefy_foundation_forecasting_model_v3e"
  decoder_checkpoint_name: ""
  decoder_input_patch_len: 32
  decoder_output_patch_len: 128
  decoder_num_layers: 10
  decoder_model_dims: 512
  decoder_num_heads: 16
  use_metadata: true
  finetune_timesfm_from_beginning: false
  finetune_timesfm_after_epochs: 100
  timeseries_token_dims: 9
  random_vector_instead_text: false
  mask_timestamp: false
  position_embedding: none
  train_epoch_length: 100
  val_epoch_length: 10
  test_epoch_length: 10
  generate_point_forecast: false
  generate_probabilistic_forecast_using_bins: true
  absolute_max_bar_value: 10 # 50
  num_bins: 1000 # 5000
  bound_output_scale: 0.0
  masking_schemes:
    - "random"
    - "block"
    - "row"
    - "train_test_block"
  # - "train_test_last"
  block_target_mask_range: 30
  block_target_mask_mean: 50
  block_mask_num: 4
  block_mask_every: false
  row_mask_num: 30
  row_mask_min: 5
  row_mask_ratio: 0.2
  attention_masking_scheme: "interpolation"

metadata_encoder_config:
  channels: ${foundation_model_config.decoder_model_dims}
  n_heads: 8
  num_encoder_layers: 6
  patch_len: ${foundation_model_config.decoder_input_patch_len}
  stride: ${foundation_model_config.decoder_input_patch_len}

execution_config:
  save_path: training_logs
  experiment_name: "tabicl"
  run_name: "synthetic_icl_sfm_v3e"
  generation_save_path: generation_logs
  use_mlflow: true
  mlflow_tracking_uri: "http://10.15.33.49:5000" # H100 - 4

training_config:
  target_mask_ratio: 0.2
  patience: 50
  learning_rate: 1e-4
  n_plots: 4
  auto_lr_find: True
  check_test_every_n_epoch: 1
  log_every_n_steps: 1
  check_test_every_n_epoch: 1000
  num_devices: 1
  val_check_interval: 1000
  check_val_every_n_epoch: 1
  strategy: "ddp_find_unused_parameters_true"
  save_all_checkpoints: false
  save_checkpoint_every_n_epochs: -1
  save_checkpoint_every_n_steps: 1000
  model_name: "foundation_model"
  model_file: "fmv3"
  num_ar_batches: 0 
  precision: "16-mixed"
  gradient_clipping: 1.0
  max_epochs: 20
  # lr_scheduler:
  #     scheduler_name: "cosine_warmup"
  #     warmup_proportion: 0.001
  #     max_steps: 20000

prior_config:
  max_classes: 10
  dataset_length: 1000
  # override_activations: "identity"

  min_train_size: 0.98
  max_train_size: 0.99

  # try out tabular parameters
  tabular_dataset_rate: 0.5
  dataset_has_timestamp_rate: 0.5

  # Try out heavy tail parameters
  scm_num_causes:
    distribution: meta_heavy_tailed
    min_mean: 0.01
    max_mean: 1.0
    min_shape: 0.8
    max_shape: 1.5
    round: true
    lower_bound: 1
  scm_num_layers:
    distribution: meta_heavy_tailed
    min_mean: 0.5
    max_mean: 5.0
    min_shape: 0.5
    max_shape: 2.0
    round: true
    lower_bound: 2
  scm_hidden_dim:
    distribution: meta_heavy_tailed_log_scaled
    min_mean: 8
    max_mean: 160
    min_shape: 0.01
    max_shape: 0.1
    round: true
    lower_bound: 10


decoder_config:
  # decoder_type: "moe"
  # # input_expert_module_names: ["linear"]
  # # embed_expert_module_names: ["linear"]
  # world_size: 1
  # n_routed_experts: 8
  # n_activated_experts: 8
  # rank: 1
  # moe_inter_dim: 256
  # bias_update_rate: 0.01
  # topk: 4
  # score_func: "softmax"
  # route_scale: 1.0

  decoder_type: "row_column"
  row_num_blocks: 3
  row_nhead: 8
  row_num_cls: 4
  row_rope_base: 10000
  icl_num_blocks: 12
  icl_nhead: 4

tabicl_config:
  embed_dim: 128 # 512
  col_num_blocks: 3 # 8
  col_nhead: 4 # 16
  col_num_inds: 128 # 512

  row_num_blocks: 3 # 8
  row_nhead: 8 # 16
  row_num_cls: 4 # 4
  row_rope_base: 10000

  icl_num_blocks: 12
  icl_nhead: 4
  preserve_col_order: true

  # external_column_embedder: "sfm_v3e"
  # external_column_embedder_config: 'examples/configs/foundation_model_configs/config_forecasting_foundation_model_v3e.yaml'
  # external_column_embedder_checkpoint: "/mnt/workspace3/synthefy_data/training_logs/covid_deaths/MLFlow_Plotting_Test/chronos_gift_pretrain_bs8_lr1e-5_1024_window_counts_block_mask_8_48_1/checkpoints/epoch_epoch=250.ckpt"

  use_time_mask: true
  time_mask_type: "sample"
  time_mask_mixing_probs: [0.6, 0.3, 0.1]
  num_layers: 2
  # weight_range: [-0.5, 0.5]
  preserve_col_order: true
  use_full_reg: true

  # random_train_mask_ratio: 0.2
  # num_layers: 3
  # weight_range: [-0.5, 0.5]
  # preserve_col_order: true