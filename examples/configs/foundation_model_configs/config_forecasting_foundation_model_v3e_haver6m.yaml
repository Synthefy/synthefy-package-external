device: "cuda"
task: "forecast"
trainer: "foundation_model"
save_key: "val_loss" #"val_loss"

# history_length = time_series_length - forecast_length
dataset_config:
  dataloader_name: V3ShardedDataloader
  dataset_name: haver6M
  v3_data_paths: ["/home/synthefy/data/fmv3_datasets/haver_6M/pretrain"]
  num_channels: 1
  time_series_length: 256
  forecast_length: 128
  metadata_length: 3460 
  timestamp_start_idx: 3
  timestamp_end_idx: 2819
  dataset_description_start_idx: 2819
  dataset_description_end_idx: 3203
  continuous_start_idx: 3203
  continuous_end_idx: 3459
  dataset_idx_start_idx: 3459
  dataset_idx_end_idx: 3460
  text_embedding_dim: 384
  batch_size: 8
  num_timestamp_features: 11
  num_correlates: 20
  num_datasets: 10000
  use_relation_shards: false
  use_window_counts: false
  num_workers: 16
  write_on_disk_cache: false
  relational_sampling_strategy: "haver_tree_directory_level_0_3"
  relational_sampling_data_location: "/home/synthefy/data/fmv3_datasets/haver_6M/pretrain/relational_sampling/haver_tree_0/0_3/"

# 170752
foundation_model_config:
  model_name: "synthefy_foundation_forecasting_model_v3e"
  decoder_checkpoint_name: ""
  decoder_input_patch_len: 32
  decoder_output_patch_len: 128
  decoder_num_layers: 15
  decoder_model_dims: 1024
  decoder_num_heads: 16
  use_metadata: true
  finetune_timesfm_from_beginning: false 
  finetune_timesfm_after_epochs: 100
  timeseries_token_dims: 9
  random_vector_instead_text: false
  mask_timestamp: false
  position_embedding: none
  train_epoch_length: 3000
  val_epoch_length: 200
  test_epoch_length: 200
  use_column_identifier: false
  block_target_mask_range: 8
  block_target_mask_mean: 48
  block_mask_num: 1
  generate_point_forecast: false
  generate_probabilistic_forecast_using_bins: true
  num_bins: 5000

  
metadata_encoder_config:
  channels: ${foundation_model_config.decoder_model_dims}
  n_heads: 8
  num_encoder_layers: 6
  patch_len: ${foundation_model_config.decoder_input_patch_len}
  stride: ${foundation_model_config.decoder_input_patch_len}

execution_config:
  save_path: training_logs
  run_name: "directory_level_p=0.3"
  generation_save_path: generation_logs
  experiment_name: HaverRelational
  use_mlflow: true
  mlflow_tracking_uri: "http://10.15.33.49:5000" # H100 - 4
  run_validation_before_training: false

training_config:
  target_mask_ratio: 0.5
  patience: 50
  learning_rate: 1e-4
  n_plots: 4
  auto_lr_find: True
  log_every_n_steps: 1
  check_test_every_n_epoch: 1000
  num_devices: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  strategy: "ddp"
  save_all_checkpoints: true
  save_checkpoint_every_n_epochs: 1
  save_checkpoint_every_n_steps: -1
  model_name: "foundation_model"
  model_file: "fmv3"
  num_ar_batches: 20
  precision: "16-mixed"