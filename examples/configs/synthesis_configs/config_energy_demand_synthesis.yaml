device: "cuda"
task: "synthesis"

# Generated by referencing docker_scripts/synthesis/ppg.sh and docker_scripts/synthesis/air_quality.sh, and their corresponding config files
dataset_config:
  dataset_name: energy_demand
  time_series_length: 32
  num_channels: 2
  num_discrete_conditions: 321
  num_discrete_labels: 20  # TODO: Look for `num_original_discrete` in the preprocess stdout.
  num_continuous_labels: 72
  latent_dim: 1024  # continous
  discrete_condition_embedding_dim: 256  # Do we want to use discrete encoder here?
  batch_size: 512

denoiser_config:
  d_model: 256 
  patch_len: 16
  stride: 8
  dropout: 0.1
  e_layers: 6
  d_layers: 1
  d_ff: 512
  n_heads: 8
  activation: "gelu"
  factor: 3
  output_attention: True
  use_metadata: True

# Same as config_forecasting_eval_air_quality.yaml#L87
metadata_encoder_config:
  use_sa_layer: True
  channels: ${denoiser_config.d_model}  # TODO: make sure this is actually being used...
  n_heads: 8
  num_encoder_layers: 2

execution_config:
  save_path: training_logs
  run_name: "synthesis_energy_demand"
  generation_save_path: generation_logs
  experiment_name: Time_Series_Diffusion_Training

training_config:
  max_epochs: 500
  learning_rate: 1e-4
  n_plots: 4
  auto_lr_find: True
  check_val_every_n_epoch: 1
  check_test_every_n_epoch: 1
  log_every_n_steps: 1
  num_devices: 1
  strategy: "auto"

# Using dataset (AWS commands)
# uv run examples/preprocess_data.py --config examples/configs/preprocessing_configs/config_energy_demand_preprocessing.json
# uv run examples/synthesize.py --config examples/configs/synthesis_configs/config_energy_demand.yaml
# uv run examples/generate_synthetic_data.py \
# --config examples/configs/synthesis_configs/config_energy_demand.yaml \
# --model_checkpoint_path $SYNTHEFY_DATASETS_BASE/training_logs/energy_demand/Time_Series_Diffusion_Training/synthesis_energy_demand/checkpoints/best_model.ckpt \
# --preprocess_config_path examples/configs/preprocessing_configs/config_energy_demand_preprocessing.json \
# --output_dir $SYNTHEFY_DATASETS_BASE/synthetic_data \
# --splits train val test
# Make sure to modify the yaml and run thrice to generate all 3: tstr, trtr, trstr
# uv run examples/tstr.py --config examples/configs/synthesis_configs/config_energy_demand.yaml

#for evaluation TRTR (train real, test real), TSTR (train synthetic, test real), TRSTR (train real, synthetic, test synthetic)
tstr_config:
  #only classification is supported for now
  train_dataset_paths: #note - only used for -> synthetic_or_original_or_custom: "custom" - point to all the files
    - path: /home/raghav/data/synthefy/runs/generation_logs/energy_demand/Time_Series_Diffusion_Training/synthesis_energy_demand/train_dataset/
      synthetic_or_original: "synthetic"

    - path: /home/raghav/data/synthefy/runs/generation_logs/energy_demand/Time_Series_Diffusion_Training/synthesis_energy_demand/train_dataset/
      synthetic_or_original: "original"

    # - path: /tmp/custom_2025-01-03_17-18-16_dataset # for non traditional output dir, it is always test_*.npy
    #   synthetic_or_original: "synthetic"

  val_dataset_paths: #note - only used for -> synthetic_or_original_or_custom: "custom" - point to all the files
    - path: /home/raghav/data/synthefy/runs/generation_logs/energy_demand/Time_Series_Diffusion_Training/synthesis_energy_demand/val_dataset/
      synthetic_or_original: "synthetic"

    - path: /home/raghav/data/synthefy/runs/generation_logs/energy_demand/Time_Series_Diffusion_Training/synthesis_energy_demand/val_dataset/
      synthetic_or_original: "original"

  test_dataset_paths: #note - only used for -> synthetic_or_original_or_custom: "custom" - point to all the files
    - path: /home/raghav/data/synthefy/runs/generation_logs/energy_demand/Time_Series_Diffusion_Training/synthesis_energy_demand/test_dataset
      synthetic_or_original: "original"


  device: cuda
  seed: 42
  num_workers: 4
  
  training:
    num_devices: 1
    learning_rate: 1e-3
    max_epochs: 20
    log_dir: /tmp/raghav/trstr_training
    check_val_every_n_epoch: 1

  synthetic_or_original_or_custom: "custom" # "original" or "synthetic" #  benchmark original against synthetic
  classification_or_regression: "classification" # "classification" or "regression"
  classifier:
      model_name: resnet1d50
  # regressor:
  #     model_name: resnet1d50

  dataset:
    num_channels: 2
    time_series_length: 32
    required_time_series_length: 32
    use_reduced_horizon: False # turn this to True when you want to use required_time_series_length as the features instead of time_series_length
    batch_size: 256
    # classification
    multi_label: False
    classification_start_index: 292  # Check the output JSON file $SYNTHEFY_DATASET_BASE/energy_demand/discrete_windows_columns.json for column indices (in this case, we are doing sky is clear in Madrid prediction based on time series data)
    classification_inclusive_end_index: 300 # Note - this is inclusive
    # regression
    index_of_interest: 28
    use_condition_input: True
    continuous_input_channels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, ] #up to num_continuous_labels
    discrete_input_channels: [0, 1, 2, 3, 4, 5, 6, 7, 8] # up to num_discrete_conditions
