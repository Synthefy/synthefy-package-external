device: "cuda"
task: "forecast"

dataset_config:
  dataset_name: electricity
  num_channels: 1
  time_series_length: 96
  forecast_length: 32
  num_discrete_labels: 1
  num_discrete_conditions: 370
  num_continuous_labels: 3
  discrete_condition_embedding_dim: 128

  latent_dim: 64
  num_timestamp_labels: 3
  use_metadata: True
  use_timestamp: True
  batch_size: 2048

patchtst_config:
  num_input_channels: 1
  context_length: 64
  prediction_length: 32
  distribution_output: "student_t"
  loss: "mse"
  # PatchTST arguments
  patch_length: 16
  patch_stride: 8
  # Transformer architecture configuration
  num_hidden_layers: 3
  d_model: 128
  num_attention_heads: 4
  share_embedding: True
  channel_attention: False
  ffn_dim: 512
  norm_type: "batchnorm"
  norm_eps: 1.0e-5
  attention_dropout: 0.0
  positional_dropout: 0.0
  path_dropout: 0.0
  ff_dropout: 0.0
  bias: true
  activation_function: "gelu"
  pre_norm: true
  positional_encoding_type: "sincos"
  use_cls_token: false
  init_std: 0.02
  share_projection: true
  scaling: "std"
  # mask pretraining
  do_mask_input: null
  mask_type: "random"
  random_mask_ratio: 0.5
  num_forecast_mask_patches: [2]
  channel_consistent_masking: false
  unmasked_channel_indices: null
  mask_value: 0
  # head
  pooling_type: "mean"
  head_dropout: 0.0
  output_range: null
  # distribution head
  num_parallel_samples: 100

execution_config:
  save_path: training_logs
  run_name: "patchtst_electricity"
  generation_save_path: generation_logs
  experiment_name: Time_Series_Diffusion_Training

training_config:
  patience: 20
  lr_scheduler:
    scheduler_name: "exponential"
    gamma: 0.999
  learning_rate: 1e-4
  n_plots: 4
  save_after_every_iters: 5
  save_after_every_n_epochs: 5
  auto_lr_find: True
  check_val_every_n_epoch: 1
  log_every_n_steps: 1
  num_devices: 1
  strategy: "auto"
