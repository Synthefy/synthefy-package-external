device: "cuda"
task: "forecast"

# {'time_series_length': 48, 'num_channels': 1, 'num_discrete_conditions': 273, 'num_continuous_labels': 4, 'num_timestamp_labels': 3}

# history_length = time_series_length - forecast_length
dataset_config:
  dataset_name: ettm1
  num_channels: 1
  time_series_length: 192
  forecast_length: 96
  num_discrete_labels: 0  # Look for `num_original_discrete` in the preprocess stdout.
  num_discrete_conditions: 0
  num_continuous_labels: 9
  discrete_condition_embedding_dim: 128

  latent_dim: 64  # continous
  num_timestamp_labels: 3
  use_metadata: True
  batch_size: 256

# These parameters are fixed for the pretrained timesfm model
timesfm_config:
  checkpoint_name: "google/timesfm-1.0-200m-pytorch"
  input_patch_len: 32
  output_patch_len: 128
  num_layers: 20
  model_dims: 1280
  num_heads: 16

execution_config:
  save_path: training_logs
  run_name: "timesfm_finetuned_ettm1"
  generation_save_path: generation_logs
  experiment_name: Time_Series_Diffusion_Training

training_config:
  patience: 20
  learning_rate: 1e-4
  n_plots: 4
  auto_lr_find: True
  check_val_every_n_epoch: 1
  log_every_n_steps: 1
  num_devices: 1
  strategy: "auto"