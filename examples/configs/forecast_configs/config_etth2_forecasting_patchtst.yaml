device: "cuda"
task: "forecast"

# {'time_series_length': 48, 'num_channels': 1, 'num_discrete_conditions': 273, 'num_continuous_labels': 4, 'num_timestamp_labels': 3}

# history_length = time_series_length - forecast_length
dataset_config:
  dataset_name: etth2
  num_channels: 1
  time_series_length: 192
  forecast_length: 96
  num_discrete_labels: 0  # Look for `num_original_discrete` in the preprocess stdout.
  num_discrete_conditions: 0
  num_continuous_labels: 9
  discrete_condition_embedding_dim: 128

  latent_dim: 64  # continous
  num_timestamp_labels: 3
  use_metadata: True
  batch_size: 256

patchtst_config:
  num_input_channels: 1
  context_length: 96
  prediction_length: 96
  distribution_output: "student_t"
  loss: "mse"
  # PatchTST arguments
  patch_length: 16
  patch_stride: 8
  # Transformer architecture configuration
  num_hidden_layers: 3
  d_model: 128
  num_attention_heads: 4
  share_embedding: True
  channel_attention: False
  ffn_dim: 512
  norm_type: "batchnorm"
  norm_eps: 1.0e-5
  attention_dropout: 0.0
  positional_dropout: 0.0
  path_dropout: 0.0
  ff_dropout: 0.0
  bias: true
  activation_function: "gelu"
  pre_norm: true
  positional_encoding_type: "sincos"
  use_cls_token: false
  init_std: 0.02
  share_projection: true
  scaling: "std"
  # mask pretraining
  do_mask_input: null
  mask_type: "random"
  random_mask_ratio: 0.5
  num_forecast_mask_patches: [2]
  channel_consistent_masking: false
  unmasked_channel_indices: null
  mask_value: 0
  # head
  pooling_type: "mean"
  head_dropout: 0.0
  output_range: null
  # distribution head
  num_parallel_samples: 100

execution_config:
  save_path: training_logs
  run_name: "patchtst_etth2"
  generation_save_path: generation_logs
  experiment_name: Time_Series_Diffusion_Training

training_config:
  patience: 20
  learning_rate: 1e-4
  n_plots: 4
  auto_lr_find: True
  check_val_every_n_epoch: 1
  log_every_n_steps: 1
  num_devices: 1
  strategy: "auto"

